# What type of training we're performing
# 0: full training (pretrained autoencoder + model)
# 1: autoencoder pretraining

training_task: 0

#--------------------------------
#       General Params
#--------------------------------
seed: [True, 1337]
train: True
model_id: 'convlstm_12M_pretrained_ae_v2'
torch_home: '/develop/pretrained_models'
exp_name: 'Test One'
num_design_params: 9

#--------------------------------
#       Network Params
#--------------------------------

# General Parameters
# - arch: Network architecture (0: MLP, 1: LSTM, 2: ConvLSTM)

arch: 2

# MLP Parameters
# - ***separate MLPs for real and imaginary parts***
# - layers: Hidden layer neuron counts, len(layers) = number of hidden layers
# - activation: Activation function for hidden layers
# - mlp_strategy: 0: full, 1: patch-wise 2: distributed subset
# - patch_size: patch height/width for patch wise or distributed subset

mlp_real:
    layers: [64, 256, 1024]
    activation: 'relu'

mlp_imag:
    layers: [64, 256, 1024]
    activation: 'relu'

mlp_strategy: 0
patch_size: 3
interpolate_fields: False

# LSTM Parameters
# - num_layers: number of lstm layers (i.e., stacked networks)
# - i_dims: number of input dimensions for lstm
# - h_dims: number of hidden dimensions for lstm

lstm:
    num_layers: 1
    i_dims: 55112 # this is (r/i * 166 * 166)
    h_dims: 256

# ConvLSTM Parameters
# - in_channels: number of input channels for conv
# - kernel_size: size of the conv kernel
# - padding: padding for the conv layer
# - use_ae: utilize autoencoder
# - pretrained_ae: use pretrained autoencoder
# - latent_dim: latent dimension for autoencoder
# - encoder_channels: encoder channel progression
# - decoder_channels: decoder channel progression

conv_lstm:
    num_layers: 1
    in_channels: 2
    out_channels: 64
    kernel_size: 5
    padding: 2
    spatial: 166
    use_ae: True
    pretrained_ae: True
    freeze_ae_weights: False
    encoder_channels: [2, 32, 64] # each step halves
    decoder_channels: [64, 32, 2] # each step doubles

# General Time Series Parameters
# - seq_len: number of time steps
# - io_mode: 'one_to_many' or 'many_to_many'
# - spacing_mode: 'sequential' or 'distributed'

seq_len: 10
io_mode: 'one_to_many'
spacing_mode: 'sequential'

#--------------------------------
#       Training Params
#--------------------------------
batch_size: 8
num_epochs: 200 # maximum

accelerator: 'gpu' 
gpu_config: [True, [0]]

valid_rate: 1

weights: 
num_classes: 1
optimizer: 'ADAM'
learning_rate: 1.e-3
lr_scheduler: 'CosineAnnealingLR'
transfer_learn: False
load_checkpoint: False
objective_function: 'mse'

# early stopping settings
patience: 30
min_delta: 0.0001

mcl_params:
    alpha: 1     #near-field
    beta: 1     #far-field
    gamma: 1    #phase
    delta: 1    #derivatives

#--------------------------------
#       All paths
#--------------------------------
#path_root: '/home/marshall/research/cai_2023'
path_root: '/develop/'
path_data: 'data/'
path_train: 'train/'
path_valid: 'valid/'
path_model: 'model_checkpoints/'
path_results: 'results/'
path_checkpoint: 'model_checkpoints/model_baseline_phases_derivatives'
path_resims: 'resims/'
path_pretrained_ae: 'results/model_autoencoder_v3/'

#--------------------------------
#       Physical Params
#--------------------------------

#Metasurface simulation
Nx_metaAtom: 3
Ny_metaAtom: 3
Lx_metaAtom: 680.e-9
Ly_metaAtom: 680.e-9

n_fusedSilica: 1.44
n_PDMS: 1.4
n_amorphousSilica: 3.48

h_pillar: 102.e-9
thickness_pml: 780.e-9
thickness_fusedSilica: 780.e-9
thickness_PDMS: 1560.e-9

#General
wavelength: 1550.e-9

#Fourier propagation
distance: 10.e-6
Nxp: 176
Nyp: 176
Lxp: 2.04e-6
Lyp: 2.04e-6
adaptive: True

#--------------------------------
#       Datamodule Params
#--------------------------------
n_cpus: 5
n_folds: 3