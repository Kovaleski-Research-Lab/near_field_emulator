# directive: Process to execute
# - 0: train network
# - 1: run evaluation
# - 2: load results/copy to local for viewing
# - 3: MEEP simulations
# - 4: data preprocessing and formatting for model
# - 5: mode encoding
# deployment: Deployment mode
# - 0: local
# - 1: kubernetes

directive: 1
deployment: 0

# - arch: Network Architecture
# *** Learning a mapping from design parameters to DFT fields ***
    # - 0: Dual MLPs - separate MLPs for real and imaginary
    # - 1: CVNN - Complex-valued MLP
# *** Learning wavefront propagation ***
    # - 2: LSTM
    # - 3: ConvLSTM
    # - 4: AE-LSTM - LSTM with linearautoencoder
    # - 5: AE-ConvLSTM - ConvLSTM with convolutional autoencoder
    # - 6: mode-LSTM - LSTM operating on a deterministic dim reduction of the fields
        # - SVD, random projection, gauss, fourier
# *** Learning a reconstrutible latent representation of the fields ***
    # - 7: Autoencoder - Used in conjunction with arch 4, 5

arch: 6

#--------------------------------
#       General Params
#--------------------------------

# General Parameters
# - seed: random seed
# - train: True/False
# - model_id: model identifier / name
# - torch_home: path to dtemp directory for holding pretrained models
# - exp_name: experiment name
# - num_design_params: number of design parameters
# - include_testing: True/Falsee - whether to test right after training
# - cross_validation: True/False - whether to perform cross-validation
# ---> if False, then a 5-fold 80/20 split is used

seed: [True, 1337]
train: True
model_id: '10-v4'
torch_home: '/develop/pretrained_models'
exp_name: 'Test One'
num_design_params: 9

#--------------------------------
#       Network Params
#--------------------------------

# MLP Parameters
# - ***separate MLPs for real and imaginary parts, or a single CVNN***
# - layers: Hidden layer neuron counts, len(layers) = number of hidden layers
# - activation: Activation function for hidden layers
# - mlp_strategy: 0: full, 1: patch-wise 2: distributed subset
# - patch_size: patch height/width for patch wise or distributed subset

mlp_real:
    layers: [64, 512]
    activation: 'relu'

mlp_imag:
    layers: [64, 512]
    activation: 'relu'

cvnn:
    layers: [64, 512]
    activation: 'complexrelu'

mlp_strategy: 0
patch_size: 3
interpolate_fields: False

# LSTM Parameters
# - num_layers: number of lstm layers (i.e., stacked networks)
# - i_dims: number of input dimensions for lstm
# - h_dims: number of hidden dimensions for lstm

lstm:
    num_layers: 1
    i_dims: 55112 # this is (r/i * 166 * 166)
    h_dims: 256

# Mode-LSTM Parameters - LSTM but data has already been encoded
# - i_dims: input dimensionality - depends on the encoding method
# ----> svd: spatial * top_k * 2
# ----> random: any perfect square
# - spatial: spatial size of the input
# - top_k: number of modes to encode
# - w0: beam waist parameter (laguerre-gaussian)
# - p_max: radial index max (laguerre-gaussian)
# - l_max: azimuthal index max (laguerre-gaussian)
# - method: encoding types ('svd' or 'random' or 'gauss' or 'fourier')

modelstm:
    num_layers: 1
    i_dims: 20
    h_dims: 64
    spatial: 166
    top_k: 10
    w0: 1.0
    p_max: 39
    l_max: 20
    seed: 1337
    method: 'svd'


# ConvLSTM Parameters
# - in_channels: number of input channels for conv
# - kernel_size: size of the conv kernel
# - padding: padding for the conv layer
# - use_ae: utilize autoencoder
# - pretrained_ae: use pretrained autoencoder
# - latent_dim: latent dimension for autoencoder
# - encoder_channels: encoder channel progression
# - decoder_channels: decoder channel progression

convlstm:
    num_layers: 1
    in_channels: 2
    out_channels: 64
    kernel_size: 5
    padding: 2
    spatial: 166

# Autoencoder Parameters
# - encoder_channels: encoder channel progression
# - decoder_channels: decoder channel progression
# - pretrained: use pretrained autoencoder
# - freeze_weights: whether to freeze weights
# - spatial: spatial size of the input
# - method: layer types ('linear' or 'conv')

autoencoder:
    encoder_channels: [2, 32, 64] # each step halves
    decoder_channels: [64, 32, 2] # each step doubles
    latent_dim: 512
    pretrained: True
    freeze_weights: False
    use_decoder: False
    spatial: 166
    method: 'linear'

# General Time Series Parameters
# - seq_len: number of time steps
# - io_mode: 'one_to_many' or 'many_to_many'
# - spacing_mode: 'sequential' or 'distributed'
# - autoreg: autoregressive mode or teacher forcing - M2M only

seq_len: 10
io_mode: 'one_to_many'
spacing_mode: 'distributed'
autoreg: True

#--------------------------------
#       Training Params
#--------------------------------
batch_size: 8
num_epochs: 100 # maximum

accelerator: 'gpu' 
gpu_config: [True, [0]]

valid_rate: 1

weights: 
num_classes: 1
optimizer: 'ADAM'
learning_rate: 1.e-3
lr_scheduler: 'CosineAnnealingLR'
transfer_learn: False
load_checkpoint: False
objective_function: 'mse'

include_testing: False
cross_validation: False

# early stopping settings
patience: 15
min_delta: 0.0001

mcl_params:
    alpha: 1     #near-field
    beta: 1     #far-field
    gamma: 1    #phase
    delta: 1    #derivatives

#--------------------------------
#       All paths
#--------------------------------
path_root: '/develop/'
path_data: 'data/preprocessed_data'
path_train: 'train/'
path_valid: 'valid/'
path_results: 'results/'
path_checkpoint: 'model_checkpoints/model_baseline_phases_derivatives'
path_volumes: 'data/nfe-data/volumes'
path_library: 'code/near_field_emulator/utils/neighbors_library_allrandom.pkl'
path_resims: 'resims/'
path_pretrained_ae: 'results/meep_meep/autoencoder/model_ae-linear-v1/'

#--------------------------------
#       Physical Params
#--------------------------------

#Metasurface simulation
Nx_metaAtom: 3
Ny_metaAtom: 3
Lx_metaAtom: 680.e-9
Ly_metaAtom: 680.e-9

n_fusedSilica: 1.44
n_PDMS: 1.4
n_amorphousSilica: 3.48

h_pillar: 102.e-9
thickness_pml: 780.e-9
thickness_fusedSilica: 780.e-9
thickness_PDMS: 1560.e-9

#General
wavelength: 1550.e-9

#Fourier propagation
distance: 10.e-6
Nxp: 176
Nyp: 176
Lxp: 2.04e-6
Lyp: 2.04e-6
adaptive: True

#--------------------------------
#       Datamodule Params
#--------------------------------
n_cpus: 5
n_folds: 3

#--------------------------------
#       Kube Params
#--------------------------------
kube:
    namespace : gpn-mizzou-muem
    image : docker.io/kovaleskilab/ml_basic:v4
    job_files : /develop/code/near_field_emulator/kube/kube_jobs  # this is a local directory
    pvc_volumes : dft-volumes # use `kubectl get pvc` to see list of pvcs
    pvc_preprocessed : preprocessed-data 
    pvc_results : training-results

    data_job:
        num_cpus : 32
        num_parallel_ops : 2
        num_mem_lim : 200Gi 
        num_mem_req : 200Gi
        kill_tag : data-job

        paths:
            # local / repo path where meta-atom radii are stored

            # interactive pod directories
            data:
                volumes: /develop/results  # points to folder containing reduced volumes in pvc called dft-volumes
                preprocessed_data: /develop/data/preprocessed_data # points to folder containing data after it has been preprocessed in pvc called preprocessed-data 
            timing: /develop/data/preprocessed_data/timing 

            # local path where template is located
            template: kube/templates/data_job.txt

    train_job :
        num_cpus : 16
        num_mem_lim : 100Gi
        num_mem_req : 100Gi
        num_gpus : 1
        kill_tags : [mlp,lstm,convlstm]
    
        paths :
            data :
                train : /develop/data/preprocessed_data/train
                valid : /develop/data/preprocessed_data/valid
            results :
                # interactive pod directories
                model_results : /develop/results
                model_checkpoints : /develop/results/checkpoints
                analysis : /develop/results/analysis
            logs : /develop/results/checkpoints/current_logs
            # local path where template is located
            template : kube/templates/train_job.txt

    load_results_job :
        num_mem_req : 64Gi
        num_mem_lim : 128Gi
        paths :
            template : templates/load_results_job.txt
            params: /develop/code/near_field_emulator/configs/params.yaml

    evaluation_job :
        paths:
            template : kube/templates/evaluation_job.txt   