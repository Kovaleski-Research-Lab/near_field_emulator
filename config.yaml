# Experiment type
# - 0: train network
# - 1: autoencoder pretraining
# - 2: data compilation (preprocessed train/valid -> .pt file)
# - 3: load results
# - 4: run evaluation

experiment: 4

#--------------------------------
#       General Params
#--------------------------------

# General Parameters
# - seed: random seed
# - train: True/False
# - model_id: model identifier / name
# - torch_home: path to dtemp directory for holding pretrained models
# - exp_name: experiment name
# - num_design_params: number of design parameters
# - include_testing: True/Falsee - whether to test right after training
# - cross_validation: True/False - whether to perform cross-validation
# ---> if False, then a 5-fold 80/20 split is used

seed: [True, 1337]
train: True
model_id: 'MSE_LRG'
torch_home: '/develop/pretrained_models'
exp_name: 'Test One'
num_design_params: 9

#--------------------------------
#       Network Params
#--------------------------------

# General Network Parameters
# - arch: Network architecture (0: MLPs, 1: CVNN, 2: LSTM, 3: ConvLSTM, 
#                               4: AE-LSTM, 5: AE-ConvLSTM)

arch: 1

# MLP Parameters
# - ***separate MLPs for real and imaginary parts, or a single CVNN***
# - layers: Hidden layer neuron counts, len(layers) = number of hidden layers
# - activation: Activation function for hidden layers
# - mlp_strategy: 0: full, 1: patch-wise 2: distributed subset
# - patch_size: patch height/width for patch wise or distributed subset

mlp_real:
    layers: [64, 512]
    activation: 'relu'

mlp_imag:
    layers: [64, 512]
    activation: 'relu'

cvnn:
    layers: [64, 512]
    activation: 'complexrelu'

mlp_strategy: 0
patch_size: 3
interpolate_fields: False

# LSTM Parameters
# - num_layers: number of lstm layers (i.e., stacked networks)
# - i_dims: number of input dimensions for lstm
# - h_dims: number of hidden dimensions for lstm

lstm:
    num_layers: 1
    i_dims: 55112 # this is (r/i * 166 * 166)
    h_dims: 256

# ConvLSTM Parameters
# - in_channels: number of input channels for conv
# - kernel_size: size of the conv kernel
# - padding: padding for the conv layer
# - use_ae: utilize autoencoder
# - pretrained_ae: use pretrained autoencoder
# - latent_dim: latent dimension for autoencoder
# - encoder_channels: encoder channel progression
# - decoder_channels: decoder channel progression

convlstm:
    num_layers: 1
    in_channels: 2
    out_channels: 64
    kernel_size: 5
    padding: 2
    spatial: 166
    use_ae: False
    pretrained_ae: True
    freeze_ae_weights: False
    encoder_channels: [2, 32, 64] # each step halves
    decoder_channels: [64, 32, 2] # each step doubles

# Autoencoder Parameters
# - encoder_channels: encoder channel progression
# - decoder_channels: decoder channel progression
# - pretrained: use pretrained autoencoder
# - freeze_weights: whether to freeze weights
# - spatial: spatial size of the input
# - modes: modes to encode ('svd' or 'standard')

autoencoder:
    encoder_channels: [2, 32, 64] # each step halves
    decoder_channels: [64, 32, 2] # each step doubles
    pretrained: True
    freeze_weights: False
    spatial: 166
    modes: 'standard'


# General Time Series Parameters
# - seq_len: number of time steps
# - io_mode: 'one_to_many' or 'many_to_many'
# - spacing_mode: 'sequential' or 'distributed'

seq_len: 15
io_mode: 'one_to_many'
spacing_mode: 'distributed'


#--------------------------------
#       Training Params
#--------------------------------
batch_size: 8
num_epochs: 50 # maximum

accelerator: 'gpu' 
gpu_config: [True, [0]]

valid_rate: 1

weights: 
num_classes: 1
optimizer: 'ADAM'
learning_rate: 1.e-3
lr_scheduler: 'CosineAnnealingLR'
transfer_learn: False
load_checkpoint: False
objective_function: 'mse'

include_testing: True
cross_validation: False

# early stopping settings
patience: 15
min_delta: 0.0001

mcl_params:
    alpha: 1     #near-field
    beta: 1     #far-field
    gamma: 1    #phase
    delta: 1    #derivatives

#--------------------------------
#       All paths
#--------------------------------
#path_root: '/home/marshall/research/cai_2023'
path_root: '/develop/'
path_data: 'data/preprocessed_data'
path_train: 'train/'
path_valid: 'valid/'
path_results: 'results/'
path_checkpoint: 'model_checkpoints/model_baseline_phases_derivatives'
path_resims: 'resims/'
path_pretrained_ae: 'results/meep_meep/autoencoder/model_ae-v1/'

#--------------------------------
#       Physical Params
#--------------------------------

#Metasurface simulation
Nx_metaAtom: 3
Ny_metaAtom: 3
Lx_metaAtom: 680.e-9
Ly_metaAtom: 680.e-9

n_fusedSilica: 1.44
n_PDMS: 1.4
n_amorphousSilica: 3.48

h_pillar: 102.e-9
thickness_pml: 780.e-9
thickness_fusedSilica: 780.e-9
thickness_PDMS: 1560.e-9

#General
wavelength: 1550.e-9

#Fourier propagation
distance: 10.e-6
Nxp: 176
Nyp: 176
Lxp: 2.04e-6
Lyp: 2.04e-6
adaptive: True

#--------------------------------
#       Datamodule Params
#--------------------------------
n_cpus: 5
n_folds: 3

#--------------------------------
#       Kube Params
#--------------------------------
kube:
    namespace : gpn-mizzou-muem
    image : docker.io/kovaleskilab/ml_basic:v4
    job_files : /develop/code/near_field_inverse_design/kube/kube_jobs  # this is a local directory
    pvc_volumes : dft-volumes # use `kubectl get pvc` to see list of pvcs
    pvc_preprocessed : preprocessed-data 
    pvc_results : training-results

    compile_job:
        num_cpus : 4
        num_mem_lim : 64Gi 
        num_mem_req : 32Gi
        kill_tag : compile-data

        paths:
            # local / repo path where meta-atom radii are stored

            # interactive pod directories
            data:
                volumes: /develop/results  # points to folder containing reduced volumes in pvc called dft-volumes
                preprocessed_data: /develop/data/preprocessed_data # points to folder containing data after it has been preprocessed in pvc called preprocessed-data 
            timing: /develop/data/preprocessed_data/timing 

            # local path where template is located
            template: kube/templates/compile_job.txt

    train_job :
        num_cpus : 16
        num_mem_lim : 100Gi
        num_mem_req : 100Gi
        num_gpus : 1
        kill_tags : [mlp,lstm,convlstm]
    
        paths :
            data :
                train : /develop/data/preprocessed_data/train
                valid : /develop/data/preprocessed_data/valid
            results :
                # interactive pod directories
                model_results : /develop/results
                model_checkpoints : /develop/results/checkpoints
                analysis : /develop/results/analysis
            logs : /develop/results/checkpoints/current_logs
            # local path where template is located
            template : kube/templates/train_job.txt

    load_results_job :
        num_mem_req : 64Gi
        num_mem_lim : 128Gi
        paths :
            template : templates/load_results_job.txt
            params: /develop/code/near_field_inverse_design/configs/params.yaml

    evaluation_job :
        paths:
            template : kube/templates/evaluation_job.txt   